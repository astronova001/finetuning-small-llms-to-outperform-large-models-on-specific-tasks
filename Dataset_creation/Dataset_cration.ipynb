{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178f14ed",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import json\n",
    "import json_parser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe665a1b",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation using OpenAI API and QnA dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "# Read the CSV file and extract the 'Paragraphs' column\n",
    "# Replace 'your_dataset.csv' with the actual path to your dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "paragraphs = df['Paragraphs']\n",
    "\n",
    "# Filter paragraphs with length greater than 1000\n",
    "filtered_paragraphs = [paragraph for paragraph in paragraphs if len(paragraph) > 1000]\n",
    "\n",
    "# Print the count of such paragraphs\n",
    "print(f\"The dataset contains {len(filtered_paragraphs)} paragraphs with a length greater than 1000.\")\n",
    "\n",
    "# Function to initialize the Azure OpenAI model\n",
    "def initilize_model():\n",
    "    \"\"\"\n",
    "    Initializes the Azure OpenAI client using environment variables for security.\n",
    "    \"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=\"https://your-azure-endpoint.openai.azure.com/\",\n",
    "        api_version=\"2024-02-01\",\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "    )\n",
    "    print(\"Model Initialized\")\n",
    "    return client\n",
    "\n",
    "# Function to perform inference on a given paragraph\n",
    "def inference(client, paragraph):\n",
    "    \"\"\"\n",
    "    Processes a paragraph using the Azure OpenAI model and generates a JSON response.\n",
    "    \"\"\"\n",
    "    # Prompt for the model\n",
    "    prompt = \"\"\"You are an advanced language model. Your task is to process a given paragraph and perform the following steps:\n",
    "    1. Extract 2-5 main key points from the paragraph. The number of key points should depend on the length of the paragraph, and the key points must be derived strictly from the paragraph itself.\n",
    "    2. Randomly remove some of the extracted key points (up to 1-4 points) from the paragraph to create a modified version of the paragraph with missing information.\n",
    "    3. Provide the missing key points that were removed.\n",
    "    4. Clean the paragraph by removing any unnecessary information, ensuring it is concise and focused.\n",
    "\n",
    "    Your output must be in the following JSON format:\n",
    "    {\n",
    "        \"Answer\": \"paragraph with missing key points\",\n",
    "        \"key_points\": [list of extracted key points],\n",
    "        \"PointsMissed\": [list of key points that were removed]\n",
    "    }\n",
    "\n",
    "    Before generating the result:\n",
    "    - Double-check the extracted key points to ensure they are accurate and strictly derived from the paragraph.\n",
    "    - Verify the modified paragraph to ensure it aligns with the missing key points and is logically consistent.\n",
    "\n",
    "    Here is the paragraph for you to process:\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the messages for the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an advanced language model. Your task is to process a given paragraph and perform the steps as instructed.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt + paragraph}\n",
    "    ]\n",
    "    print(\"Processing Paragraph:\", paragraph)\n",
    "    print(\"Generating JSON Response...\")\n",
    "\n",
    "    # Call the Azure OpenAI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"test-gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=900\n",
    "    )\n",
    "\n",
    "    # Ensure the response content is valid\n",
    "    content = response.choices[0].message.content\n",
    "    if not content.strip():\n",
    "        raise ValueError(\"The response content is empty.\")\n",
    "    return content\n",
    "\n",
    "# Function to parse and save the JSON response\n",
    "def parse_json(test_data, json_string, output_file):\n",
    "    \"\"\"\n",
    "    Parses the JSON string returned by the model, appends additional data, and saves it to a file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean up the JSON string\n",
    "        json_string = json_string.strip()\n",
    "        if json_string.startswith(\"```json\") and json_string.endswith(\"```\"):\n",
    "            json_string = json_string[7:-3].strip()\n",
    "        elif json_string.startswith(\"```\") and json_string.endswith(\"```\"):\n",
    "            json_string = json_string[3:-3].strip()\n",
    "        \n",
    "        # Parse the JSON string\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Add the test_data (original paragraph) to the JSON data\n",
    "        json_data = {\"testmessage\": test_data, **json_data}\n",
    "        \n",
    "        # Append the parsed JSON data to the output file\n",
    "        if os.path.exists(output_file):\n",
    "            with open(output_file, 'r+') as file:\n",
    "                try:\n",
    "                    existing_data = json.load(file)\n",
    "                    if isinstance(existing_data, list):\n",
    "                        existing_data.append(json_data)\n",
    "                    else:\n",
    "                        existing_data = [existing_data, json_data]\n",
    "                except json.JSONDecodeError:\n",
    "                    existing_data = [json_data]\n",
    "                file.seek(0)\n",
    "                json.dump(existing_data, file, indent=4)\n",
    "        else:\n",
    "            with open(output_file, 'w') as file:\n",
    "                json.dump([json_data], file, indent=4)\n",
    "        \n",
    "        print(f\"JSON data successfully appended to {output_file}\")\n",
    "        return json_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    client = initilize_model()\n",
    "    \n",
    "    # Define the output file\n",
    "    output_file = 'output.json'\n",
    "\n",
    "    # Process each filtered paragraph\n",
    "    for paragraph in filtered_paragraphs:\n",
    "        test_data = paragraph\n",
    "        try:\n",
    "            # Perform inference\n",
    "            result = inference(client, test_data)\n",
    "            \n",
    "            # Parse and save the result\n",
    "            if result:\n",
    "                parse_json(test_data, result, output_file)\n",
    "            else:\n",
    "                print(\"Error: No valid result to parse.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping paragraph due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f60470",
   "metadata": {},
   "source": [
    "### Extracting 300 examples for Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "with open('output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=300, random_state=42)\n",
    "\n",
    "# Save the training data to a JSON file\n",
    "with open('train_data.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Save the testing data to a JSON file\n",
    "with open('test_data.json', 'w') as test_file:\n",
    "    json.dump(test_data, test_file, indent=4)\n",
    "\n",
    "print(\"Training and testing datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098354b8",
   "metadata": {},
   "source": [
    "### Convert Existing Training Dataset to alpaca prompt template Format so it can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48108f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instruction = \"Identify key points missing from the given answer.\"\n",
    "\n",
    "def format_training_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        formatted_item = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": { \n",
    "                'Answer': item[\"Answer\"],\n",
    "                \"Key_Points\": item[\"key_points\"]\n",
    "            },\n",
    "            \"output\": {\n",
    "                'Points_Missed': item[\"PointsMissed\"]\n",
    "            }\n",
    "        }\n",
    "        formatted_data.append(formatted_item)\n",
    "    return formatted_data\n",
    "\n",
    "# Load the training data\n",
    "train_data_path = 'train_data.json'  # Ensure this points to a valid JSON file\n",
    "train_data = json.load(open(train_data_path))\n",
    "\n",
    "# Format the training data\n",
    "formatted_data = format_training_data(train_data)\n",
    "\n",
    "# Save the formatted data to a new JSON file\n",
    "with open('formatted_train_data.json', 'w') as f:\n",
    "    json.dump(formatted_data, f, indent=4)\n",
    "\n",
    "print(\"Formatted training data saved to formatted_train_data.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
