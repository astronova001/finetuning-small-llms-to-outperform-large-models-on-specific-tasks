{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679b8f3d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Kaggle Optimization Note\n",
    "\n",
    "This notebook is optimized for execution on Kaggle.\n",
    "\n",
    "**Benefits of using Kaggle:**\n",
    "*   **Data Persistence:** Kaggle environments allow you to save your work, datasets, and trained models, which persists between sessions. This is a significant advantage over Google Colab's free tier where data is often lost when the runtime disconnects.\n",
    "*   **GPU Resources:** Kaggle provides free access to GPUs, including the T4, for up to 30 hours per week, which is suitable for training models like the one in this notebook.\n",
    "*   **Integrated Datasets:** Easily access and use datasets hosted on Kaggle.\n",
    "\n",
    "While Google Colab might offer faster runtime environments in some cases, the data persistence and generous GPU quota on Kaggle make it a suitable platform for training tasks where saving progress and models is important. This notebook leverages these Kaggle features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea439fd0",
   "metadata": {
    "papermill": {
     "duration": 0.004772,
     "end_time": "2025-04-07T10:30:27.091695",
     "exception": false,
     "start_time": "2025-04-07T10:30:27.086923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5185017",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:10:17.464346Z",
     "iopub.status.busy": "2025-04-07T08:10:17.463935Z",
     "iopub.status.idle": "2025-04-07T08:13:36.263185Z",
     "shell.execute_reply": "2025-04-07T08:13:36.262021Z",
     "shell.execute_reply.started": "2025-04-07T08:10:17.464324Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-07T10:30:27.096549",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41b4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:13:36.264840Z",
     "iopub.status.busy": "2025-04-07T08:13:36.264533Z",
     "iopub.status.idle": "2025-04-07T08:13:36.421864Z",
     "shell.execute_reply": "2025-04-07T08:13:36.421292Z",
     "shell.execute_reply.started": "2025-04-07T08:13:36.264803Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manage your secrets from the \"Add-ons\" menu in the top navigation of the editor\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Set your HF token & username as environment variables\n",
    "# This step is optional for most models. However, if you are training large models with A100 \n",
    "# or accessing restricted models on Hugging Face that require authentication, you need this.\n",
    "os.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "# Replace with your username\n",
    "os.environ[\"HF_USERNAME\"] = \"Your_userName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae660d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:13:36.423523Z",
     "iopub.status.busy": "2025-04-07T08:13:36.423299Z",
     "iopub.status.idle": "2025-04-07T08:14:33.202131Z",
     "shell.execute_reply": "2025-04-07T08:14:33.201377Z",
     "shell.execute_reply.started": "2025-04-07T08:13:36.423473Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\", # You can choose any model from the above list that works on both colab and Kaggle.\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae446d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:14:33.203424Z",
     "iopub.status.busy": "2025-04-07T08:14:33.203178Z",
     "iopub.status.idle": "2025-04-07T08:14:39.727206Z",
     "shell.execute_reply": "2025-04-07T08:14:39.726534Z",
     "shell.execute_reply.started": "2025-04-07T08:14:33.203404Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA rank. Suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # Modules to apply LoRA\n",
    "    lora_alpha = 16, # Scaling factor for LoRA\n",
    "    lora_dropout = 0, # Dropout rate for LoRA. 0 is optimized for no dropout\n",
    "    bias = \"none\",    # Bias type. \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # Use gradient checkpointing for memory efficiency. \"unsloth\" is optimized\n",
    "    random_state = 3407, # Random seed for reproducibility\n",
    "    use_rslora = False,  # Enable rank-stabilized LoRA (optional)\n",
    "    loftq_config = None, # Configuration for LoftQ quantization (optional)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36d124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:14:39.728470Z",
     "iopub.status.busy": "2025-04-07T08:14:39.728164Z",
     "iopub.status.idle": "2025-04-07T08:14:39.733535Z",
     "shell.execute_reply": "2025-04-07T08:14:39.732766Z",
     "shell.execute_reply.started": "2025-04-07T08:14:39.728441Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the Alpaca-style prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Retrieve the End-of-Sequence (EOS) token from the tokenizer\n",
    "# The EOS token is crucial as it marks the end of a sequence, helping the model\n",
    "# understand where the input or output ends. This is especially important for\n",
    "# tasks like text generation to ensure proper formatting and termination.\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Function to format the dataset examples into the Alpaca-style prompt\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for example in examples[\"data\"]:  # Iterate through the list of dictionaries\n",
    "        instruction = example[\"instruction\"]  # Extract the instruction\n",
    "        input_text = example[\"input\"]         # Extract the input context\n",
    "        output = example[\"output\"]            # Extract the expected output\n",
    "        # Format the text using the Alpaca prompt template and append the EOS token\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
    "        texts.append(text)  # Add the formatted text to the list\n",
    "    return {\"text\": texts}  # Return the formatted texts as a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637f066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:14:39.734506Z",
     "iopub.status.busy": "2025-04-07T08:14:39.734235Z",
     "iopub.status.idle": "2025-04-07T08:14:47.146119Z",
     "shell.execute_reply": "2025-04-07T08:14:47.145389Z",
     "shell.execute_reply.started": "2025-04-07T08:14:39.734462Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset from a JSON file\n",
    "new_dataset = json.load(open('/kaggle/input/formatted_train_data.json'))\n",
    "\n",
    "# Convert list to a Hugging Face Dataset if needed\n",
    "if isinstance(new_dataset, list):\n",
    "    new_dataset = Dataset.from_dict({\"data\": new_dataset})\n",
    "\n",
    "# Format the dataset using the Alpaca-style prompt function\n",
    "dataset = new_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Save the processed Hugging Face dataset to disk for future use\n",
    "dataset.save_to_disk(\"/kaggle/working/formatted_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd0119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:14:47.147246Z",
     "iopub.status.busy": "2025-04-07T08:14:47.146981Z",
     "iopub.status.idle": "2025-04-07T08:14:47.152612Z",
     "shell.execute_reply": "2025-04-07T08:14:47.151946Z",
     "shell.execute_reply.started": "2025-04-07T08:14:47.147225Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb364a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:14:49.596237Z",
     "iopub.status.busy": "2025-04-07T08:14:49.595908Z",
     "iopub.status.idle": "2025-04-07T08:14:51.967197Z",
     "shell.execute_reply": "2025-04-07T08:14:51.966457Z",
     "shell.execute_reply.started": "2025-04-07T08:14:49.596201Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer  # Import SFTTrainer for supervised fine-tuning\n",
    "from transformers import TrainingArguments  # Import training arguments\n",
    "from unsloth import is_bfloat16_supported  # Check if bfloat16 is supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,  # Pre-trained model\n",
    "    tokenizer = tokenizer,  # Tokenizer for text processing\n",
    "    train_dataset = dataset,  # Training dataset\n",
    "    dataset_text_field = \"text\",  # Field containing text data\n",
    "    max_seq_length = max_seq_length,  # Max sequence length for inputs\n",
    "    dataset_num_proc = 2,  # Number of processes for dataset preprocessing\n",
    "    packing = False,  # Disable sequence packing for simplicity\n",
    "    args = TrainingArguments(  # Training configuration\n",
    "        per_device_train_batch_size = 2,  # Batch size per device\n",
    "        gradient_accumulation_steps = 4,  # Steps to accumulate gradients\n",
    "        warmup_steps = 5,  # Warmup steps for learning rate\n",
    "        num_train_epochs = 1,  # Number of training epochs\n",
    "        max_steps = 60,  # Max training steps\n",
    "        learning_rate = 2e-4,  # Learning rate\n",
    "        fp16 = not is_bfloat16_supported(),  # Use fp16 if bfloat16 unsupported\n",
    "        bf16 = is_bfloat16_supported(),  # Use bfloat16 if supported\n",
    "        logging_steps = 1,  # Log every step\n",
    "        optim = \"adamw_8bit\",  # Optimizer with 8-bit precision\n",
    "        weight_decay = 0.01,  # Weight decay for regularization\n",
    "        lr_scheduler_type = \"linear\",  # Linear learning rate scheduler\n",
    "        seed = 3407,  # Random seed for reproducibility\n",
    "        output_dir = \"outputs\",  # Directory to save outputs\n",
    "        report_to = \"none\",  # Disable reporting (e.g., WandB)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965557b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:14:51.968830Z",
     "iopub.status.busy": "2025-04-07T08:14:51.968471Z",
     "iopub.status.idle": "2025-04-07T08:23:43.700592Z",
     "shell.execute_reply": "2025-04-07T08:23:43.699893Z",
     "shell.execute_reply.started": "2025-04-07T08:14:51.968784Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21910e76",
   "metadata": {},
   "source": [
    "## Test / Inference the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0403bf3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Normal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336dd376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:23:43.701802Z",
     "iopub.status.busy": "2025-04-07T08:23:43.701448Z",
     "iopub.status.idle": "2025-04-07T08:23:48.054500Z",
     "shell.execute_reply": "2025-04-07T08:23:48.053561Z",
     "shell.execute_reply.started": "2025-04-07T08:23:43.701767Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Identify key points missing from the given answer\", # instruction\n",
    "        '''\"Answer\": \"The documentation of health impacts from ambient air pollution has been challenging due to exposure assessment complexities, confounding factors like smoking, infections, and allergies, and the intricacies of studying large populations. Recently, sophisticated global studies have conclusively shown that air pollution significantly affects human health. Respiratory symptoms, notably complicating chronic bronchitis, are the predominant adverse effects across different pollution types. Air pollution is linked to elevated risks of heart and lung disease mortality, even at sub-toxic levels. While there's limited evidence of air pollution as a primary cause of cancer, certain emission sources may contribute to cancer risk, especially in exceptional cases.\",\n",
    "        \"key_points\": [\n",
    "            \"Exposure assessment complexities and confounding factors make studying health impacts of air pollution challenging.\",\n",
    "            \"Global studies have shown that air pollution significantly affects human health.\",\n",
    "            \"Respiratory symptoms, such as chronic bronchitis, are predominant adverse effects of air pollution.\",\n",
    "            \"Air pollution is linked to elevated risks of heart and lung disease mortality, even at sub-toxic levels.\",\n",
    "            \"Mucosal irritation, including bronchitis, nasal irritation, and conjunctivitis, occurs with high pollution exposure.\",\n",
    "            \"Eye irritation can be severe due to particulates or high concentrations of photochemical oxidants and aldehydes.\",\n",
    "            \"Certain emission sources may contribute to cancer risk, though evidence of air pollution as a primary cause of cancer is limited.\"\n",
    "        ]''', # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6296f83",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Stream Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd341e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T08:23:48.055738Z",
     "iopub.status.busy": "2025-04-07T08:23:48.055425Z",
     "iopub.status.idle": "2025-04-07T08:24:00.200726Z",
     "shell.execute_reply": "2025-04-07T08:24:00.199821Z",
     "shell.execute_reply.started": "2025-04-07T08:23:48.055716Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Identify key points missing from the given answer look for all the points carefully and print nothing else and think twice before you answer and give the most appropriate answer\", # instruction\n",
    "        ''' \"Answer\": \"The International Atomic Energy Agency's Marine Environmental Laboratory (IAEA-MEL) has played a vital role in ensuring the quality of marine radioactivity measurements for nearly three decades. Through numerous global and regional intercomparison exercises and the supply of reference materials, Analytical Quality Control Services (AQCS) has emerged as a critical component of the IAEA's mission. Recognizing the increasing importance of environmental data in economic, ecological, and legal decision-making, the IAEA-MEL is evaluating ways to further enhance data quality. Proposed enhancements include a comprehensive approach to total quality management for analytical laboratories, encompassing quality assurance programs and manuals, intercomparison exercises with effective feedback mechanisms, and laboratory accreditation.\",\n",
    "        \"key_points\": [\n",
    "            \"IAEA-MEL has ensured the quality of marine radioactivity measurements for nearly three decades.\",\n",
    "            \"Analytical Quality Control Services (AQCS) is a critical component of the IAEA's mission.\",\n",
    "            \"IAEA-MEL organized 41 intercomparison exercises for radionuclides and produced 35 reference materials.\",\n",
    "            \"Environmental data is increasingly important in economic, ecological, and legal decision-making.\",\n",
    "            \"Proposed enhancements include total quality management for analytical laboratories.\",\n",
    "            \"Certified reference material production according to ISO standards is part of the proposed enhancements.\",\n",
    "            \"Quality assessment and control training is included in the initiatives to improve data quality.\"\n",
    "        ]''', # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622ed49",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T08:09:46.078Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# # model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# # tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cd903",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T08:09:46.079Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### To Download the Weights for future use run the below code and download the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3000f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T08:09:46.079Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"/kaggle/working/qwen2.5_march_lora\", 'zip', \"/kaggle/working/lora_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7033008,
     "sourceId": 11296182,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7030555,
     "sourceId": 11296206,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 289536,
     "modelInstanceId": 268514,
     "sourceId": 318210,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 289850,
     "modelInstanceId": 268824,
     "sourceId": 318578,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T10:30:23.804467",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
