{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a180a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27645b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Your_finetuned_model\", # Can also use normal models\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def format_input(answer, key_points):\n",
    "    \"\"\"Formats input text for inference.\"\"\"\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "Compare the given answer with the key points and identify which key points are missing from the answer. Return only the list of missing key points in JSON format as {{\"Points_Missed\": [list of missing points]}}.\n",
    "\n",
    "### Input:\n",
    "Answer: \"{answer}\"\n",
    "\n",
    "Key points that should be included:\n",
    "{json.dumps(key_points, indent=2)}\n",
    "\n",
    "### Response:\n",
    "The missing key points are:\n",
    "\"\"\"\n",
    "\n",
    "def generate_missing_points(json_input, model, tokenizer, stream=False):\n",
    "    \"\"\"Generates missing key points using the model.\"\"\"\n",
    "    answer, key_points, reference_output = json_input[\"Answer\"], json_input[\"key_points\"], json_input['PointsMissed']\n",
    "    formatted_input = format_input(answer, key_points)\n",
    "    \n",
    "    inputs = tokenizer([formatted_input], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=150, use_cache=True)\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the response part\n",
    "    response = decoded_output[0].split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    # Try multiple parsing approaches\n",
    "    try:\n",
    "        # First, try to find JSON structure with {\"Points_Missed\": [...]}\n",
    "        match = re.search(r'{\"Points_Missed\":\\s*\\[.*?\\]}', response, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(0), json.dumps({\"Points_Missed\": reference_output}, indent=4)\n",
    "            \n",
    "        # Second, try to find a list structure\n",
    "        match = re.search(r'\\[(.*?)\\]', response, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                points_list = json.loads(match.group(0))\n",
    "                return json.dumps({\"Points_Missed\": points_list}, indent=4), json.dumps({\"Points_Missed\": reference_output}, indent=4)\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to clean and parse the list\n",
    "                list_str = match.group(0)\n",
    "                # Replace single quotes with double quotes for JSON compatibility\n",
    "                list_str = list_str.replace(\"'\", '\"')\n",
    "                try:\n",
    "                    points_list = json.loads(list_str)\n",
    "                    return json.dumps({\"Points_Missed\": points_list}, indent=4), json.dumps({\"Points_Missed\": reference_output}, indent=4)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # If no structure found, try to extract points line by line\n",
    "        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "        # Remove numbered lists (1. Point)\n",
    "        cleaned_lines = [re.sub(r'^\\d+\\.\\s*', '', line) for line in lines]\n",
    "        # Remove any lines that are too short or look like headings\n",
    "        valid_points = [line for line in cleaned_lines if len(line) > 10 and not line.isupper()]\n",
    "        \n",
    "        return json.dumps({\"Points_Missed\": valid_points}, indent=4), json.dumps({\"Points_Missed\": reference_output}, indent=4)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing output: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        # Fallback to empty list\n",
    "        return json.dumps({\"Points_Missed\": []}, indent=4), json.dumps({\"Points_Missed\": reference_output}, indent=4)\n",
    "\n",
    "def calculate_metrics(predicted_json, reference_json):\n",
    "    \"\"\"Calculates precision, recall, and F1-score between predicted and reference JSONs.\"\"\"\n",
    "    try:\n",
    "        # Convert JSON strings to Python dictionaries\n",
    "        if isinstance(predicted_json, str):\n",
    "            predicted = json.loads(predicted_json)[\"Points_Missed\"]\n",
    "        else:\n",
    "            predicted = predicted_json[\"Points_Missed\"]\n",
    "            \n",
    "        if isinstance(reference_json, str):\n",
    "            reference = json.loads(reference_json)[\"Points_Missed\"]\n",
    "        else:\n",
    "            reference = reference_json[\"Points_Missed\"]\n",
    "\n",
    "        # Convert to sets for comparison\n",
    "        predicted_set = set(predicted)\n",
    "        reference_set = set(reference)\n",
    "\n",
    "        # Calculate precision, recall, and F1-score\n",
    "        true_positives = len(predicted_set & reference_set)\n",
    "        false_positives = len(predicted_set - reference_set)\n",
    "        false_negatives = len(reference_set - predicted_set)\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        return {\n",
    "            \"precision\": 0,\n",
    "            \"recall\": 0,\n",
    "            \"f1_score\": 0,\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"false_negatives\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def evaluate_examples(examples, model, tokenizer, output_file=\"evaluation_results.json\"):\n",
    "    \"\"\"Evaluate all examples and save results to a file.\"\"\"\n",
    "    all_metrics = []\n",
    "    individual_results = []\n",
    "    \n",
    "    for i, example in enumerate(tqdm(examples, desc=\"Processing examples\")):\n",
    "        try:\n",
    "            generated_output, reference_output = generate_missing_points(example, model, tokenizer)\n",
    "            metrics = calculate_metrics(generated_output, reference_output)\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            # Store individual results\n",
    "            individual_results.append({\n",
    "                \"example_id\": i,\n",
    "                \"answer_text\": example.get('Answer', '')[:100] + \"...\",  # Store first 100 chars\n",
    "                \"generated\": json.loads(generated_output) if isinstance(generated_output, str) else generated_output,\n",
    "                \"reference\": json.loads(reference_output) if isinstance(reference_output, str) else reference_output,\n",
    "                \"metrics\": metrics\n",
    "            })\n",
    "            \n",
    "            if (i+1) % 5 == 0:\n",
    "                print(f\"Processed {i+1}/{len(examples)} examples\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i+1}: {str(e)}\")\n",
    "            individual_results.append({\n",
    "                \"example_id\": i,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    if all_metrics:\n",
    "        avg_precision = np.mean([m[\"precision\"] for m in all_metrics])\n",
    "        avg_recall = np.mean([m[\"recall\"] for m in all_metrics])\n",
    "        avg_f1 = np.mean([m[\"f1_score\"] for m in all_metrics])\n",
    "        \n",
    "        avg_metrics = {\n",
    "            \"avg_precision\": float(avg_precision),\n",
    "            \"avg_recall\": float(avg_recall),\n",
    "            \"avg_f1_score\": float(avg_f1)\n",
    "        }\n",
    "    else:\n",
    "        avg_metrics = {\n",
    "            \"avg_precision\": 0.0,\n",
    "            \"avg_recall\": 0.0,\n",
    "            \"avg_f1_score\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Save results to a file\n",
    "    results = {\n",
    "        \"average_metrics\": avg_metrics,\n",
    "        \"individual_results\": individual_results\n",
    "    }\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    return avg_metrics, individual_results\n",
    "\n",
    "# Main function for Kaggle environment\n",
    "def main():\n",
    "    # Path to the test dataset\n",
    "    input_file = \"/kaggle/input/test-dataset/test.json\"\n",
    "    \n",
    "    # Output file for results\n",
    "    output_file = \"/kaggle/working/evaluation_results.json\"\n",
    "    \n",
    "    \n",
    "    # Load examples from JSON file\n",
    "    with open(input_file, 'r') as f:\n",
    "        examples = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(examples)} examples from {input_file}\")\n",
    "    \n",
    "    # Evaluate examples\n",
    "    avg_metrics, individual_results = evaluate_examples(examples, model, tokenizer, output_file)\n",
    "    \n",
    "    # Print average metrics\n",
    "    print(\"\\n=== AVERAGE METRICS ===\")\n",
    "    print(f\"Average Precision: {avg_metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1-score: {avg_metrics['avg_f1_score']:.4f}\")\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
